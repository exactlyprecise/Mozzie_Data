{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gray/anaconda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import os\n",
    "from csv import DictReader\n",
    "import numpy as np\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'Untitled.ipynb', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'daily_tabulated_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-32561dc66610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load daily csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdaily_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"daily_tabulated_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdaily_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'daily_tabulated_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# load daily csv file\n",
    "daily_df = pd.read_csv(\"daily_tabulated_data.csv\")\n",
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column headers\n",
    "col_names = []\n",
    "for col in daily_df:\n",
    "    col_names.append(col)\n",
    "\n",
    "#label_header = col_names[3]\n",
    "#print(label_header)\n",
    "temperature_headers = col_names[4:24]\n",
    "print(temperature_headers)\n",
    "print(len(temperature_headers))\n",
    "rainfall_headers = col_names[24:44]\n",
    "print(rainfall_headers)\n",
    "print(len(rainfall_headers))\n",
    "pop_headers = col_names[44:64]\n",
    "print(pop_headers)\n",
    "print(len(pop_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load daily data into list of numpy\n",
    "station_data = []\n",
    "\n",
    "for station_idx in range(20):\n",
    "    #station_data.append([])\n",
    "    data = []\n",
    "    data.append(daily_df[temperature_headers[station_idx]].tolist()[3654:None])\n",
    "    data.append(daily_df[rainfall_headers[station_idx]].tolist()[3654:None])\n",
    "    #data.append(daily_df[\"total-population\"].tolist()[3654:None])\n",
    "    #data.append(daily_df[pop_headers[station_idx]].tolist())\n",
    "    data = np.array(data)\n",
    "    station_data.append(data)\n",
    "    #print(data.shape)\n",
    "    \n",
    "    \n",
    "assert(len(station_data) == 20)\n",
    "for station_idx in range(20):\n",
    "    assert(station_data[0].shape[0]==2)\n",
    "    #print(station_data[0].shape[1])\n",
    "    assert(station_data[0].shape[1]==3612)\n",
    "print(\"All daily sizes correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weekly csv file\n",
    "weekly_df = pd.read_csv(\"weekly_tabulated_data.csv\")\n",
    "weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth and weekly pop data\n",
    "groundtruth_list = weekly_df[\"dengue-incidence\"].tolist()[522:None]\n",
    "groundtruth = np.array(groundtruth_list)\n",
    "pop_list = weekly_df[\"total-population\"].tolist()[522:None]\n",
    "pop = np.array(pop_list)\n",
    "\n",
    "print(groundtruth.shape[0])\n",
    "print(pop.shape[0])\n",
    "assert(groundtruth.shape[0] == 1038 - 522)\n",
    "assert(pop.shape[0] == 1038 - 522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_margin):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    data - numpy array of shape (N, num_feats)\n",
    "    train_margin - ratio of datapoints allocated to train data\n",
    "\n",
    "    Returns:\n",
    "    train_data - numpy array of shape (floor(N * train margin), num_feats)\n",
    "    test_data - numpy array of shape (N - floor(N * train margin), num_feats)\n",
    "    \n",
    "    Note:\n",
    "    Exception handling not implemented\n",
    "    \"\"\"\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    print(n)\n",
    "    num_training = floor(n * train_margin)\n",
    "    return data[0:num_training,:], data[num_training:n,:]\n",
    "\n",
    "\n",
    "def train_test_split_predetermined_weekly(weekly_data):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    weekly_data - numpy array of shape (M, num_feats)\n",
    "    \n",
    "    Conditions:\n",
    "    M = 516\n",
    "    \n",
    "    train dataset size = 359 weeks\n",
    "    test dataset = 157 weeks\n",
    "    \"\"\"\n",
    "    weekly_data_train, weekly_data_test = weekly_data[0:359,:], weekly_data[359:None,:]\n",
    "    print(weekly_data_train.shape)\n",
    "    print(weekly_data_test.shape)\n",
    "    return weekly_data_train, weekly_data_test\n",
    "\n",
    "def train_test_split_predetermined_daily(daily_data):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    daily_data - numpy array of shape (N, num_feats)\n",
    "    \n",
    "    Conditions:\n",
    "    N = 3612\n",
    "    \n",
    "    train dataset size = 2513 days\n",
    "    test dataset = 1099 days\n",
    "    \"\"\"\n",
    "    daily_data_train, daily_data_test = daily_data[0:2513,:], daily_data[2513:None,:]\n",
    "    print(daily_data_train.shape)\n",
    "    print(daily_data_test.shape)\n",
    "    return daily_data_train, daily_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_daily, test_daily = train_test_split(station_data[0].T, 0.8)\n",
    "# train_weekly, test_weekly = train_test_split(np.expand_dims(pop,axis=1), 0.80045)\n",
    "# print(station_data[0].T.shape)\n",
    "# print(train_daily.shape)\n",
    "# print(test_daily.shape)\n",
    "# print(\"\")\n",
    "\n",
    "# print(np.expand_dims(pop,axis=1).shape)\n",
    "# print(train_weekly.shape)\n",
    "# print(test_weekly.shape)\n",
    "\n",
    "# print(train_daily.shape[0] / train_weekly.shape[0])\n",
    "\n",
    "weekly_data_train, weekly_data_test = train_test_split_predetermined_weekly(np.expand_dims(pop,axis=1))\n",
    "groundtruth_train, groundtruth_test = train_test_split_predetermined_weekly(np.expand_dims(groundtruth,axis=1))\n",
    "\n",
    "station_data_train = []\n",
    "station_data_test = []\n",
    "\n",
    "for i in range(len(station_data)):\n",
    "    daily_train, daily_test = train_test_split_predetermined_daily(station_data[i].T)\n",
    "    station_data_train.append(daily_train)\n",
    "    station_data_test.append(daily_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data):\n",
    "    \"\"\"Standardizes data across axis 0. Returns the mean and stddev of each column as well.\n",
    "    Parameters:\n",
    "    data - numpy array of shape (N, num_feats)\n",
    "    \n",
    "    Returns:\n",
    "    std_data - data standardized across axis 0\n",
    "    mean - mean of each column. array of shape (num_feats)\n",
    "    stddev - standard deviation of each column. array of shape (num_feats)\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = np.sum(data, axis=0) / data.shape[0]\n",
    "    stddev = np.std(data, axis=0)\n",
    "    std_data = (data - mean) / stddev\n",
    "    return std_data, mean, stddev\n",
    "\n",
    "def standardize_data_using_precomputed(data, mean, stddev):\n",
    "    return (data - mean) / stddev\n",
    "\n",
    "def undo_standardization(data, mean, stddev):\n",
    "    return data * stddev + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK RESERVED FOR STANDARDIZATION OF DATA\n",
    "\n",
    "weekly_data_train, weekly_mean, weekly_std = standardize_data(weekly_data_train)\n",
    "weekly_data_test = standardize_data_using_precomputed(weekly_data_test, weekly_mean, weekly_std)\n",
    "\n",
    "#groundtruth_train, groundtruth_mean, groundtruth_std = standardize_data(groundtruth_train)\n",
    "#groundtruth_test = standardize_data_using_precomputed(groundtruth_test, groundtruth_mean, groundtruth_std)\n",
    "\n",
    "groundtruth_train = groundtruth_train / 400\n",
    "\n",
    "groundtruth_test = groundtruth_test / 400\n",
    "\n",
    "#print(undo_standardization(groundtruth_train, groundtruth_mean, groundtruth_std))\n",
    "\n",
    "station_means = []\n",
    "station_stds = []\n",
    "for i in range(len(station_data)):\n",
    "    station_data_train[i], _mean, _std = standardize_data(station_data_train[i])\n",
    "    station_means.append(_mean)\n",
    "    station_stds.append(_std)\n",
    "    station_data_test[i] = standardize_data_using_precomputed(station_data_test[i], _mean, _std)\n",
    "    \n",
    "#print(station_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_the_data(time_series_data, window_size):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    time_series_data - numpy array of shape (N, num_feats)\n",
    "    window_size - int representing window size\n",
    "    \n",
    "    Returns:\n",
    "    numpy array of shape (N-window_size, window_size, num_feats). Each row represents one datapoint.\n",
    "    \n",
    "    NOTE: WE REDUCE BY ONE EXTRA DATAPOINT THAN IS NECESSARY TO PRESERVE MULTIPLICITY OF 7 IN DAILY DATA.\n",
    "    \"\"\"\n",
    "    time_series_data = time_series_data.T\n",
    "    num_feats = time_series_data.shape[0]\n",
    "    col_dim = time_series_data.shape[1]\n",
    "    resultant_n = col_dim - window_size +1\n",
    "    datapoints = []\n",
    "    for i in range(resultant_n):\n",
    "        single_datapoint = []\n",
    "        for feat in range(num_feats):\n",
    "            single_datapoint.append(time_series_data[feat, i:i+window_size])\n",
    "        #print(len(single_datapoint))\n",
    "        datapoints.append(single_datapoint)\n",
    "    return np.transpose(np.array(datapoints), axes=(0,2,1))[1:None]\n",
    "\n",
    "def process_data_subset(station_data, weekly_data, groundtruth, weekly_window, prediction_lead=8):\n",
    "    \"\"\"Windows the data, then crops the groundtruth to match the training or test dataset after windowing. \n",
    "    Then crops all data to process for prediction_lead time.\n",
    "    \n",
    "    eg. For T+8, first 7 groundtruths are deleted, and last 7 weekly entries of data deleted.\n",
    "    for daily data, last 49 entries are deleted.\n",
    "    \"\"\"\n",
    "    windowed_station_data = []\n",
    "    for i in range(len(station_data_train)):\n",
    "        windowed_station_data.append(window_the_data(station_data[i], weekly_window*7)[:-((prediction_lead-1)*7)])\n",
    "        \n",
    "    windowed_weekly_data = window_the_data(weekly_data, weekly_window)[:-(prediction_lead-1)]\n",
    "    windowed_groundtruth = groundtruth[weekly_window:-(prediction_lead-1)]\n",
    "    \n",
    "    return windowed_station_data, windowed_weekly_data, windowed_groundtruth\n",
    "\n",
    "\n",
    "def convert_all_station_daily_to_weekly_series(station_data):\n",
    "    \"\"\"Groups all station data by week\n",
    "    \n",
    "    Input: List of station data. Each element in the list should be a numpy array of shape (N_daily, num_feats)\n",
    "    Returns: List of converted station data. Each element is numpy array of shape (N//7, 7, num_feats)\n",
    "    \"\"\"\n",
    "    \n",
    "    weekly_station_data = []\n",
    "    \n",
    "    for i in range(len(station_data)):\n",
    "        new_data = []\n",
    "        num_feats = station_data[i].shape[1]\n",
    "        \n",
    "        #print(station_data[i].shape[0]//7)\n",
    "        #print(num_feats)\n",
    "        new_weekly_data = []\n",
    "        for j in range(station_data[i].shape[0]//7):\n",
    "            new_data = []\n",
    "            for k in range(num_feats):\n",
    "                new_data.append(station_data[i][j*7:j*7+7,k])\n",
    "                #print(np.array(new_data).shape)\n",
    "                \n",
    "            #print(\"appending\")\n",
    "            new_weekly_data.append(new_data)\n",
    "            \n",
    "        #print(len(new_weekly_data))\n",
    "        weekly_station_data.append(np.transpose(np.array(new_weekly_data), axes=(0,2,1)))\n",
    "        \n",
    "    return weekly_station_data\n",
    "\n",
    "def window_converted_daily_station_data(station_data, weekly_window):\n",
    "    windowed_station_data = []\n",
    "    \n",
    "    for i in range(len(station_data)):\n",
    "        num_feats = station_data[i].shape[2]\n",
    "        num_points = station_data[i].shape[0]\n",
    "        resultant_N = num_points - weekly_window + 1\n",
    "        \n",
    "        datapoints = []\n",
    "        for j in range(resultant_N):\n",
    "            single_datapoint = []\n",
    "            for feat in range(num_feats):\n",
    "                single_datapoint.append(list(station_data[i][j,:,feat]))\n",
    "                for k in range(1, weekly_window):\n",
    "                    single_datapoint[feat].extend(station_data[i][j+k,:,feat])\n",
    "            datapoints.append(single_datapoint)\n",
    "        windowed_station_data.append(np.transpose(np.array(datapoints)[1:None], axes=(0,2,1)))\n",
    "    return windowed_station_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups all daily data into groups of 7 to match weekly batch dimensionality.\n",
    "# Thereafter, apply windowing to the resultant data.\n",
    "# Difference between grouping and windowing is that there are no overlaps in grouping, while windowing has overlaps\n",
    "\n",
    "\n",
    "# window the daily data\n",
    "WEEKLY_WINDOW = 26\n",
    "#DAILY_WINDOW = 7 * WEEKLY_WINDOW\n",
    "\n",
    "converted_station_data_train = convert_all_station_daily_to_weekly_series(station_data_train)\n",
    "print(\"Converted Train:\", converted_station_data_train[0].shape)\n",
    "#print(list(converted_station_data_train[0][0,:,1]))\n",
    "\n",
    "windowed_conv_station_data_train = window_converted_daily_station_data(converted_station_data_train, WEEKLY_WINDOW)\n",
    "print(\"Windowed Train:\", windowed_conv_station_data_train[0].shape)\n",
    "\n",
    "converted_station_data_test = convert_all_station_daily_to_weekly_series(station_data_test)\n",
    "print(\"Converted Test:\", converted_station_data_test[0].shape)\n",
    "#print(list(converted_station_data_test[0][0,:,1]))\n",
    "\n",
    "windowed_conv_station_data_test = window_converted_daily_station_data(converted_station_data_test, WEEKLY_WINDOW)\n",
    "print(\"Windowed Test:\", windowed_conv_station_data_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, window the population data and process labels\n",
    "\n",
    "windowed_weekly_data_train = window_the_data(weekly_data_train, WEEKLY_WINDOW)\n",
    "print(\"Windowed population data train:\", windowed_weekly_data_train.shape)\n",
    "\n",
    "windowed_weekly_data_test = window_the_data(weekly_data_test, WEEKLY_WINDOW)\n",
    "print(\"Windowed population data test:\", windowed_weekly_data_test.shape)\n",
    "\n",
    "windowed_groundtruth_train = groundtruth_train[WEEKLY_WINDOW:None] \n",
    "windowed_groundtruth_test = groundtruth_test[WEEKLY_WINDOW:None] \n",
    "print(\"Windowed groundtruth train:\", windowed_groundtruth_train.shape)\n",
    "print(\"Windowed groundtruth train:\", windowed_groundtruth_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, offset by prediction lead time. \n",
    "# This entails deleting the first (PREDICTION - 1) points of training data,\n",
    "# and deleting the last (PREDICTION - 1) of groundtruth labels\n",
    "\n",
    "PREDICTION_LEAD = 8 # weeks\n",
    "\n",
    "final_station_train = []\n",
    "final_station_test = []\n",
    "\n",
    "for i in range(len(windowed_conv_station_data_train)):\n",
    "    final_station_train.append(windowed_conv_station_data_train[i][0:-(PREDICTION_LEAD-1)])\n",
    "    final_station_test.append(windowed_conv_station_data_test[i][0:-(PREDICTION_LEAD-1)])\n",
    "\n",
    "print(\"Final station train (Single):\", final_station_train[0].shape)\n",
    "print(\"Final station test (Single):\", final_station_test[0].shape)\n",
    "\n",
    "final_pop_train = windowed_weekly_data_train[0:-(PREDICTION_LEAD-1)]\n",
    "final_pop_test = windowed_weekly_data_test[0:-(PREDICTION_LEAD-1)]\n",
    "\n",
    "print(\"Final population train:\", final_pop_train.shape)\n",
    "print(\"Final population test:\", final_pop_test.shape)\n",
    "\n",
    "final_groundtruth_train = windowed_groundtruth_train[(PREDICTION_LEAD - 1):None]\n",
    "final_groundtruth_test = windowed_groundtruth_test[(PREDICTION_LEAD - 1):None]\n",
    "\n",
    "print(\"Final groundtruth train:\", final_groundtruth_train.shape)\n",
    "print(\"Final groundtruth test:\", final_groundtruth_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # window the daily data\n",
    "# WEEKLY_WINDOW = 52\n",
    "# DAILY_WINDOW = 7 * WEEKLY_WINDOW\n",
    "\n",
    "\n",
    "# windowed_station_data_train, windowed_weekly_data_train, windowed_groundtruth_train = process_data_subset(station_data_train, \n",
    "#                                                                                                           weekly_data_train, \n",
    "#                                                                                                           groundtruth_train,\n",
    "#                                                                                                           WEEKLY_WINDOW)\n",
    "# print(len(windowed_station_data_train))\n",
    "# print(windowed_station_data_train[0].shape)\n",
    "# print(windowed_weekly_data_train.shape)\n",
    "# print(windowed_groundtruth_train.shape)\n",
    "# print(\"\")\n",
    "\n",
    "# windowed_station_data_test, windowed_weekly_data_test, windowed_groundtruth_test = process_data_subset(station_data_test, \n",
    "#                                                                                                           weekly_data_test, \n",
    "#                                                                                                           groundtruth_test,\n",
    "#                                                                                                           WEEKLY_WINDOW)\n",
    "\n",
    "# print(len(windowed_station_data_test))\n",
    "# print(windowed_station_data_test[0].shape)\n",
    "# print(windowed_weekly_data_test.shape)\n",
    "# print(windowed_groundtruth_test.shape)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # window the daily data\n",
    "# WEEKLY_WINDOW = 52\n",
    "# DAILY_WINDOW = 7 * WEEKLY_WINDOW\n",
    "\n",
    "# windowed_station_data_train = []\n",
    "# windowed_station_data_test = []\n",
    "\n",
    "# for i in range(len(station_data_train)):\n",
    "#     windowed_station_data_train.append(window_the_data(station_data_train[i], DAILY_WINDOW))\n",
    "#     windowed_station_data_test.append(window_the_data(station_data_test[i], DAILY_WINDOW))\n",
    "# print(windowed_station_data_train[0].shape)\n",
    "# print(windowed_station_data_test[0].shape)\n",
    "\n",
    "\n",
    "# windowed_weekly_data_train = window_the_data(weekly_data_train, WEEKLY_WINDOW)\n",
    "# windowed_weekly_data_test = window_the_data(weekly_data_test, WEEKLY_WINDOW)\n",
    "\n",
    "# print(windowed_weekly_data_train.shape)\n",
    "# print(windowed_weekly_data_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_station_daily_networks(station_daily_data):\n",
    "    station_network_submodels = []\n",
    "    station_network_submodels_inputs = []\n",
    "    station_network_submodels_outputs = []\n",
    "    \n",
    "    window_size = station_daily_data[0].shape[1]\n",
    "    #print(window_size)\n",
    "    num_feats = station_daily_data[0].shape[2]\n",
    "    \n",
    "    for i in range(len(station_daily_data)):\n",
    "        inputs = keras.layers.Input(shape=(window_size, num_feats))\n",
    "        layer = keras.layers.Conv1D(filters=4, kernel_size=28, padding='same', activation='relu')(inputs)\n",
    "        layer = keras.layers.Conv1D(filters=4, kernel_size=14, padding='same', activation='relu')(layer)\n",
    "        layer = keras.layers.Conv1D(filters=8, kernel_size=7, padding='same', activation='relu')(layer)\n",
    "        layer = keras.layers.Conv1D(filters=16, kernel_size=7, strides=7, padding='valid', activation='relu')(layer)\n",
    "        #layer = keras.layers.Conv1D(filters=8, kernel_size=7, strides=7, padding='valid', activation='relu')(inputs)\n",
    "        \n",
    "        station_network_submodels.append(keras.models.Model(inputs=inputs, outputs=layer))\n",
    "        station_network_submodels_inputs.append(station_network_submodels[i].input)\n",
    "        station_network_submodels_outputs.append(station_network_submodels[i].output)\n",
    "        \n",
    "    return station_network_submodels, station_network_submodels_inputs, station_network_submodels_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_submods, submods_inputs, submods_outputs = create_station_daily_networks(final_station_train)\n",
    "\n",
    "print(station_submods[0].summary())\n",
    "\n",
    "aggregated_outputs = keras.layers.concatenate(submods_outputs)\n",
    "\n",
    "print(aggregated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_inputs = keras.layers.Input(shape=(final_pop_train.shape[1], final_pop_train.shape[2]))\n",
    "print(pop_inputs)\n",
    "\n",
    "combined_with_population = keras.layers.concatenate([aggregated_outputs, pop_inputs])\n",
    "print(combined_with_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inputs = submods_inputs\n",
    "final_inputs.append(pop_inputs)\n",
    "print(len(final_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_layer = keras.layers.Conv1D(filters=8, kernel_size=28, padding='same', activation='relu')(combined_with_population)\n",
    "#end_layer = keras.layers.MaxPooling1D(pool_size=2)(end_layer)\n",
    "\n",
    "end_layer = keras.layers.Conv1D(filters=16, kernel_size=14, padding='same', activation='relu')(end_layer)\n",
    "end_layer = keras.layers.Conv1D(filters=16, kernel_size=14, padding='same', activation='relu')(end_layer)\n",
    "end_layer = keras.layers.MaxPooling1D(pool_size=2)(end_layer)\n",
    "\n",
    "end_layer = keras.layers.Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(end_layer)\n",
    "# end_layer = keras.layers.MaxPooling1D(pool_size=2)(end_layer)\n",
    "\n",
    "end_layer = keras.layers.Flatten()(end_layer)\n",
    "end_layer = keras.layers.Dense(128, activation=\"relu\", activity_regularizer=keras.regularizers.l1(0.001))(end_layer)\n",
    "end_layer = keras.layers.Dense(64, activation=\"relu\",  activity_regularizer=keras.regularizers.l1(0.001))(end_layer)\n",
    "end_layer = keras.layers.Dense(32, activation=\"relu\",  activity_regularizer=keras.regularizers.l1(0.001))(end_layer)\n",
    "end_layer = keras.layers.Dense(1, activation=\"linear\",  activity_regularizer=keras.regularizers.l1(0.001))(end_layer)\n",
    "\n",
    "final_model = keras.models.Model(inputs=final_inputs, outputs=end_layer)\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = keras.optimizers.Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "opt = keras.optimizers.SGD(lr=0.0001, nesterov=True)\n",
    "#sgd = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "#ck_save = keras.callbacks.ModelCheckpoint('best_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "#reduce_lr_loss = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n",
    "\n",
    "final_model.compile(optimizer=sgd\n",
    "                    ,loss='mean_squared_error')\n",
    "                    #,callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input_to_train_model = final_station_train\n",
    "final_input_to_train_model.append(final_pop_train)\n",
    "\n",
    "print(len(final_input_to_train_model), type(final_input_to_train_model))\n",
    "\n",
    "final_input_to_val_model = final_station_test\n",
    "final_input_to_val_model.append(final_pop_test)\n",
    "\n",
    "print(len(final_input_to_val_model), type(final_input_to_val_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 326 samples, validate on 124 samples\n",
      "Epoch 1/120\n",
      "326/326 [==============================] - 14s 44ms/step - loss: 0.6719 - val_loss: 0.4372\n",
      "Epoch 2/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6673 - val_loss: 0.4317\n",
      "Epoch 3/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6606 - val_loss: 0.4255\n",
      "Epoch 4/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6535 - val_loss: 0.4191\n",
      "Epoch 5/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6457 - val_loss: 0.4127\n",
      "Epoch 6/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6383 - val_loss: 0.4067\n",
      "Epoch 7/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6318 - val_loss: 0.4010\n",
      "Epoch 8/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6249 - val_loss: 0.3957\n",
      "Epoch 9/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6188 - val_loss: 0.3906\n",
      "Epoch 10/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6130 - val_loss: 0.3859\n",
      "Epoch 11/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6075 - val_loss: 0.3814\n",
      "Epoch 12/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.6019 - val_loss: 0.3773\n",
      "Epoch 13/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5970 - val_loss: 0.3733\n",
      "Epoch 14/120\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.5925 - val_loss: 0.3696\n",
      "Epoch 15/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5882 - val_loss: 0.3662\n",
      "Epoch 16/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5838 - val_loss: 0.3628\n",
      "Epoch 17/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5800 - val_loss: 0.3597\n",
      "Epoch 18/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5761 - val_loss: 0.3568\n",
      "Epoch 19/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5726 - val_loss: 0.3540\n",
      "Epoch 20/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5695 - val_loss: 0.3513\n",
      "Epoch 21/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5656 - val_loss: 0.3488\n",
      "Epoch 22/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5627 - val_loss: 0.3464\n",
      "Epoch 23/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5597 - val_loss: 0.3441\n",
      "Epoch 24/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5565 - val_loss: 0.3419\n",
      "Epoch 25/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5538 - val_loss: 0.3397\n",
      "Epoch 26/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5510 - val_loss: 0.3376\n",
      "Epoch 27/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5483 - val_loss: 0.3356\n",
      "Epoch 28/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5457 - val_loss: 0.3336\n",
      "Epoch 29/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5426 - val_loss: 0.3316\n",
      "Epoch 30/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5402 - val_loss: 0.3297\n",
      "Epoch 31/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5377 - val_loss: 0.3279\n",
      "Epoch 32/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5353 - val_loss: 0.3261\n",
      "Epoch 33/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5330 - val_loss: 0.3244\n",
      "Epoch 34/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5306 - val_loss: 0.3227\n",
      "Epoch 35/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5280 - val_loss: 0.3211\n",
      "Epoch 36/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5259 - val_loss: 0.3194\n",
      "Epoch 37/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5236 - val_loss: 0.3178\n",
      "Epoch 38/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5213 - val_loss: 0.3162\n",
      "Epoch 39/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.5189 - val_loss: 0.3147\n",
      "Epoch 40/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5168 - val_loss: 0.3132\n",
      "Epoch 41/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5145 - val_loss: 0.3117\n",
      "Epoch 42/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5124 - val_loss: 0.3102\n",
      "Epoch 43/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5101 - val_loss: 0.3088\n",
      "Epoch 44/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5083 - val_loss: 0.3074\n",
      "Epoch 45/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.5059 - val_loss: 0.3059\n",
      "Epoch 46/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5038 - val_loss: 0.3045\n",
      "Epoch 47/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.5018 - val_loss: 0.3031\n",
      "Epoch 48/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.3018\n",
      "Epoch 49/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4981 - val_loss: 0.3005\n",
      "Epoch 50/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4961 - val_loss: 0.2991\n",
      "Epoch 51/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4941 - val_loss: 0.2978\n",
      "Epoch 52/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4923 - val_loss: 0.2966\n",
      "Epoch 53/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4904 - val_loss: 0.2953\n",
      "Epoch 54/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4885 - val_loss: 0.2940\n",
      "Epoch 55/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4864 - val_loss: 0.2928\n",
      "Epoch 56/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4849 - val_loss: 0.2916\n",
      "Epoch 57/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4831 - val_loss: 0.2904\n",
      "Epoch 58/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4815 - val_loss: 0.2893\n",
      "Epoch 59/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4799 - val_loss: 0.2881\n",
      "Epoch 60/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4782 - val_loss: 0.2870\n",
      "Epoch 61/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4767 - val_loss: 0.2859\n",
      "Epoch 62/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4750 - val_loss: 0.2849\n",
      "Epoch 63/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4733 - val_loss: 0.2838\n",
      "Epoch 64/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4719 - val_loss: 0.2828\n",
      "Epoch 65/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4703 - val_loss: 0.2818\n",
      "Epoch 66/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4690 - val_loss: 0.2807\n",
      "Epoch 67/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4674 - val_loss: 0.2798\n",
      "Epoch 68/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4660 - val_loss: 0.2788\n",
      "Epoch 69/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4646 - val_loss: 0.2778\n",
      "Epoch 70/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4631 - val_loss: 0.2769\n",
      "Epoch 71/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4618 - val_loss: 0.2760\n",
      "Epoch 72/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4603 - val_loss: 0.2751\n",
      "Epoch 73/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4591 - val_loss: 0.2742\n",
      "Epoch 74/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4578 - val_loss: 0.2733\n",
      "Epoch 75/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4564 - val_loss: 0.2724\n",
      "Epoch 76/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4551 - val_loss: 0.2716\n",
      "Epoch 77/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4538 - val_loss: 0.2707\n",
      "Epoch 78/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4526 - val_loss: 0.2698\n",
      "Epoch 79/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4512 - val_loss: 0.2690\n",
      "Epoch 80/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4499 - val_loss: 0.2681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4485 - val_loss: 0.2673\n",
      "Epoch 82/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4473 - val_loss: 0.2665\n",
      "Epoch 83/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4460 - val_loss: 0.2657\n",
      "Epoch 84/120\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4447 - val_loss: 0.2649\n",
      "Epoch 85/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4434 - val_loss: 0.2641\n",
      "Epoch 86/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4421 - val_loss: 0.2633\n",
      "Epoch 87/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4409 - val_loss: 0.2625\n",
      "Epoch 88/120\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.4397 - val_loss: 0.2618\n",
      "Epoch 89/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4384 - val_loss: 0.2610\n",
      "Epoch 90/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4372 - val_loss: 0.2602\n",
      "Epoch 91/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4360 - val_loss: 0.2595\n",
      "Epoch 92/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4348 - val_loss: 0.2587\n",
      "Epoch 93/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4336 - val_loss: 0.2580\n",
      "Epoch 94/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4324 - val_loss: 0.2573\n",
      "Epoch 95/120\n",
      "326/326 [==============================] - 1s 5ms/step - loss: 0.4313 - val_loss: 0.2566\n",
      "Epoch 96/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4300 - val_loss: 0.2559\n",
      "Epoch 97/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4289 - val_loss: 0.2552\n",
      "Epoch 98/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4278 - val_loss: 0.2545\n",
      "Epoch 99/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4266 - val_loss: 0.2538\n",
      "Epoch 100/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4255 - val_loss: 0.2532\n",
      "Epoch 101/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4242 - val_loss: 0.2525\n",
      "Epoch 102/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4232 - val_loss: 0.2519\n",
      "Epoch 103/120\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.4222 - val_loss: 0.2512\n",
      "Epoch 104/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4210 - val_loss: 0.2506\n",
      "Epoch 105/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4200 - val_loss: 0.2500\n",
      "Epoch 106/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4189 - val_loss: 0.2493\n",
      "Epoch 107/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4178 - val_loss: 0.2487\n",
      "Epoch 108/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4167 - val_loss: 0.2481\n",
      "Epoch 109/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4158 - val_loss: 0.2475\n",
      "Epoch 110/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4147 - val_loss: 0.2469\n",
      "Epoch 111/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4137 - val_loss: 0.2464\n",
      "Epoch 112/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4126 - val_loss: 0.2458\n",
      "Epoch 113/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4117 - val_loss: 0.2452\n",
      "Epoch 114/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4106 - val_loss: 0.2447\n",
      "Epoch 115/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4097 - val_loss: 0.2441\n",
      "Epoch 116/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4087 - val_loss: 0.2436\n",
      "Epoch 117/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4077 - val_loss: 0.2431\n",
      "Epoch 118/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4067 - val_loss: 0.2426\n",
      "Epoch 119/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4058 - val_loss: 0.2420\n",
      "Epoch 120/120\n",
      "326/326 [==============================] - 2s 5ms/step - loss: 0.4048 - val_loss: 0.2415\n"
     ]
    }
   ],
   "source": [
    "history = final_model.fit(final_input_to_train_model,\n",
    "                                  final_groundtruth_train,\n",
    "                                  validation_data=(final_input_to_val_model, final_groundtruth_test),\n",
    "                                  batch_size=50,\n",
    "                                  epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2174e0e1e7cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_input_to_val_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#predicted_unstandardize = undo_standardization(predicted, groundtruth_mean, groundtruth_std)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_unstandardize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_unstandardize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predicted = final_model.predict(final_input_to_val_model)\n",
    "#predicted_unstandardize = undo_standardization(predicted, groundtruth_mean, groundtruth_std)\n",
    "predicted_unstandardize = predicted * 400\n",
    "print(predicted_unstandardize)\n",
    "\n",
    "x_axis = range(len(predicted_unstandardize))\n",
    "\n",
    "plt.plot(x_axis, predicted_unstandardize)\n",
    "plt.plot(x_axis, final_groundtruth_test * 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
